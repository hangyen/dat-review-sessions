{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Workflow Review\n",
    "\n",
    "First thing to know - there are no hard and fast rules in data science. When you're a beginner, it's kind of like throwing spaghetti at a wall and seeing what sticks. If you're more advanced, it's still like throwing spaghetti at a wall but you'll be able to judge what sticks and what might fall off better. So it's all about iteration, trying new things, getting creative, and tuning your model until you run out of time or money :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "You need to know these concepts very well! This is the foundation that you need to understand all the things you do to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continous vs Categorical**\n",
    "- Continuous - measurement on continuous scale - age, height, money\n",
    "- Categorical - discrete variables - yes/no, on/off, happy/sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features vs observations vs target**\n",
    "- Features => columns\n",
    "- Observations => rows\n",
    "- Target => column that you're trying to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised vs Unsupervised learning**\n",
    "- Supervised => you have the answers in your dataset\n",
    "- Unsupervised => you don't know the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression vs Classification**\n",
    "- Regression => predict relationship between variables - good to predict for continuous variables\n",
    "- Classification => predict categories, puts data in buckets i.e. is this a malignant or benign tumor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some of our Supervised models:\n",
    "\n",
    "## Regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "# Some other ones you could look into:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "## Classification \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Some other ones you could look into:\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some of our Unsupervised models:\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Some other ones you could look into:\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, hierarchical, linkage_tree\n",
    "from sklearn.neighbors import LSHForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting vs Underfitting aka Bias-Variance Tradeoff**\n",
    "- https://www.kdnuggets.com/2016/08/bias-variance-tradeoff-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Step by Step Process\n",
    "\n",
    "The key here is to always use your brain!!! You can get lost in all the different tools and techniques that we show you but you need to remember to think about the problem you're trying to solve before diving into these tools and techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Get the data**\n",
    "- How much data do you have? ROT: >300 observations is minimum \n",
    "- What kind of data is it? Is it mostly text? Mostly continuous? Mostly categorical? Time based? A combo? Any ordinals that you have to take into account?\n",
    "- What do you want to use this data to predict? Would this be a supervised learning model or unsupervised learning model?\n",
    "- If supervised, is this going to be a regression or classification problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load the data**\n",
    "- Is your data csv? json? some other file? coming from a database?\n",
    "- Different type of files require different methods to load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('[file]')\n",
    "\n",
    "# from tab separated file: (This can be any delimeter!)\n",
    "df = pd.read_csv('[file].txt', sep='\\t')\n",
    "\n",
    "# from URL:\n",
    "df = pd.read_csv('http:// ... /[file].csv')\n",
    "\n",
    "df = pd.read_json('[json object]')\n",
    "df = pd.read_parquet('[parquet file]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Test Train Split**\n",
    "- Why here? Because it prevents data leakage!\n",
    "\n",
    "**What is Data leakage??**\n",
    "\n",
    "Let's say for instance you decide to test train split after you finish cleaning, feature engineering and scaling. There are multiple ways that this causes data leakage:\n",
    " - You impute nulls based on the mean -- you found the mean by using the the whole dataset, not just the training data set - you've leaked the mean from the test set into your train set\n",
    " - You do a standard scalar - you're using the std of the whole dataset, not just the train set - you've leaked the std from your test set to your train set\n",
    " - You create new features by adding the median of a column to each row - your median would be different if you were just using the train set -- you've leaked the median from your test set to your train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# You can also make this stratified!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput missing values\n",
    "X_train['column_missing_values'].fillna(X_train['columncolumn_missing_values'].median(), inplace=True)\n",
    "X_test['column_missing_values'].fillna(X_train['columncolumn_missing_values'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Clean & Explore the Data (only on the train set)**\n",
    "\n",
    "Why do we do this??\n",
    "\n",
    "This part should inform your feature engineering! What are the relationships you see between features? Can you combine them? What new features would you want to make from the data that you currently have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for nulls\n",
    "    - If you have a lot of nulls in one feature/row - maybe best to just get rid of that feature/row in your model\n",
    "    - If you have only 5% nulls in a feature/row - maybe better to impute\n",
    "    - Understand how your impute methods will affect your data \n",
    "    - Do you want to impute with the mean? the median? some other value? why/why not?\n",
    "    - `df.isnull().sum()` is helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the distribution\n",
    "    - If you're doing a linear regression, you want distributions to be normal. Why??\n",
    "    - Histograms are helpful here!\n",
    "    - Remember - before you graph, you must clean your data for nulls. Graphing also does not work if there's text in your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for outliers -> decide what to do with outliers\n",
    "    - You can get rid of them - know how this will affect your data\n",
    "    - You can keep some of them - know how this will affect your data\n",
    "    - Anything else you can do with outliers?\n",
    "    - Boxplots are helpful here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for correlations\n",
    "    - `df.corr()` is helpful here\n",
    "    - This gives you how correlated two features are to each other\n",
    "    - Why is this important? \n",
    "    - `sns.heatmap()` is helpful here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for colinearity\n",
    "    - How correlated are your features to EACH other (not to the target)\n",
    "    - How *many* of these features are correlated to EACH other?\n",
    "    - If you find colinearity, you can either pick features to put into the model that are NOT correlated with each other \n",
    "    - OR usually the case is you have many features that are correlated to each other OR if you have just a TON of features that you want to include - you would want to now keep in the back of your mind that you are gonna use a regularized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Feature Engineering**\n",
    "\n",
    "Why do we feature engineer???\n",
    "- What other features might I want?\n",
    "    - Create new features - you're given year a house is built, maybe you want to subtract that from 2018 to find house age\n",
    "    - Log, square, cube a feature - why? because it might give you a normal distribution or just have better correlation to your target\n",
    "    - Group the feature into a binary - ordinal scales 1-5 (>3 is good is 1, <3 is bad =0)\n",
    "    - Make dummies! Why? `pd.get_dummies()` is helpful here, remember to always remove the last column in a df of dummies. Why?\n",
    "    - There are many ways that you can feature engineer - this is the fun part, where creativity comes in and this is the part where you will revisit most often when you find that your model is not performing so hot. Think about what features would influence your predictor, use your brain!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Modeling!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create your target (y) variable and your feature matrix (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = admissions_data['admit']\n",
    "X = admissions_data.drop('admit', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may also want to define certain columns in your feature set already, so maybe you have something like:\n",
    "features = ['num_rooms','num_bathrooms', ..., 'num_rooms_div_bathrooms']\n",
    "X = admissions_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **To Scale or Not to Scale** \n",
    "\n",
    "**Why and when should I scale?** \n",
    "\n",
    "*Annoying answer - It depends!*\n",
    "\n",
    "- Scaling normalizes your features - this means that if you had one feature set that had a range from 10K- 100K, and a feature set from 1 to 10, scaling will put them on the same range and not put undue importance on the larger feature set.\n",
    "    - Example - sqft of bathrooms vs sqft of bedrooms \n",
    "    - Generally, if you're using models that use distance/magnitude, you want to scale:\n",
    "        - in most cases, you will scale except for decision trees\n",
    "        - When in doubt - scale, it doesn't hurt.\n",
    "    \n",
    "**How should I scale?**\n",
    "\n",
    "*Annoying answer - It depends!*\n",
    " \n",
    " Two most common ways are: Standard Scalar and Min Max scalar\n",
    "    \n",
    "When do I use `Standard Scalar`?\n",
    "- Centers your data around 0, aka z-score normalization - good for comparing measurements with different units\n",
    "- More popular method \n",
    "    \n",
    "When do I use Min Max Scaler?\n",
    "- Scales everything between 0 and 1\n",
    "- Makes std smaller, suppresses effects of outliers\n",
    "- Good for image processing\n",
    "\n",
    "Read more: \n",
    "- http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#about-min-max-scaling\n",
    "- http://rajeshmahajan.com/standard-scaler-v-min-max-scaler-machine-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling! When is this important?!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Notice, like the rest of Sklearn, it is a class!\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "\n",
    "# Notice we fit and transform on the X_train, but we ONLY transform the test.\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit\n",
    "\n",
    "What model are you planning to use and why? \n",
    "- Understanding strengths and weaknesses of each algorithm is very important:\n",
    "https://elitedatascience.com/machine-learning-algorithms\n",
    "- Never fit on your test set!!! Why?\n",
    "\n",
    "\n",
    "**Always think about what your loss function is!**\n",
    "http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "\n",
    "**If you are curious, take a look at some math of optimizers!**  \n",
    "* SGD Flavor: http://ruder.io/optimizing-gradient-descent/\n",
    "* Non SGD Guide (some SGD is in there though...): https://www.scipy-lectures.org/advanced/mathematical_optimization/index.html\n",
    "\n",
    "\n",
    "**To regularize or not to regularize?**\n",
    "\n",
    "What is regularization? Why do we do it?\n",
    "- It helps prevent overfitting\n",
    "- It's a method that penalizes the loss function so that it can find the most optimal coefficients\n",
    "- Only available for linear and logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256.0, 196.25, 145.0, 102.25, 68.0, 42.25, 25.0, 16.25, 16.0, 24.25, 41.0, 66.25, 100.0, 142.25, 193.0, 252.25, 320.0, 396.25, 481.0, 574.25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10bac9b70>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOBW9iEiEU9GLiES4/w80Gdux+mZ1fQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1086930b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#illustrating regularization:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = 2\n",
    "y_predicted = 20\n",
    "b0 = 4\n",
    "alpha = .5\n",
    "\n",
    "error_lst = []\n",
    "for b1 in range(0, 20):\n",
    "    error = (y_predicted - b1*x - b0)**2 + (alpha*b1)**2\n",
    "    error_lst.append(error)\n",
    "print(error_lst)\n",
    "\n",
    "plt.plot(error_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines!\n",
    "_A quick aside_\n",
    "\n",
    "* These are a nice wrapper around any objects that follow the sklearn class structure (ie. with .fit() and .transform())  \n",
    "* http://queirozf.com/entries/scikit-learn-pipeline-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# it takes a list of tuples as parameter\n",
    "pipeline = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# use the pipeline object as you would\n",
    "# a regular classifier\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Gridsearch or not to Gridsearch?**\n",
    "\n",
    "**What is gridsearch?**\n",
    "- It's a method to iteratively tune our model and see which are the best hyperparameter values for our model\n",
    "\n",
    "**Why do we gridsearch?** \n",
    "- It lets us tune hyperparameters easily so we don't have to manually loop through hyperparameters\n",
    "\n",
    "**What are some hyperparameters?**\n",
    "- alpha/C - regularization\n",
    "- k - number of neighbors for knn\n",
    "- number of clusters - kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = param_grid = {\n",
    "    'C':[0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), cv=3, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "preds = grid.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Another quick aside on Pipelines!_\n",
    "\n",
    "You can use pipelines AND GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# this is where you define the values for\n",
    "# GridSearchCV to iterate over\n",
    "param_grid = {\n",
    "    'clf__C':[0.01,0.1,1.0]\n",
    "}\n",
    "\n",
    "# do 3-fold cross validation for each of the 6 possible\n",
    "# combinations of the parameter values above\n",
    "grid = GridSearchCV(pipeline, cv=3, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To cross_validate or not to cross_validate?**\n",
    "\n",
    "*Answer:* Always cross validate!\n",
    "\n",
    "**What is cross_validation?**\n",
    "- Another method of doing test train split\n",
    "\n",
    "**Why do we do cross_validation?**\n",
    "- So that we can see if our model is underfitted or overfitted\n",
    "- Allows for better tuning before we use our test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Score & Interpret your Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the metric we use to measure regression? How about classification?**\n",
    "\n",
    "- Regressions - R2 score\n",
    "- Classficiations - accuracy\n",
    "\n",
    "**How do you interpret an R2 score?**\n",
    "- R2 = .83 => \"the model explains 83% of the variance in the data\"\n",
    "- Is that good or bad? => the closer to 1 the better\n",
    "- In linear regressions, how do you interpret coefficients? How about rmse? \n",
    "\n",
    "**How do you interpret an accuracy score?**\n",
    "- Accuracy = .90 => the model is correct 90% of the time\n",
    "- Is that good or bad? Depends on the baseline!!\n",
    "\n",
    "**What if you cross validated and your scores are all over the place?**\n",
    "- That means your model is overfitted!\n",
    "- What do you do then? - Go back to Step 5 - Feature Engineering!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check this page out:** http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Some of our supervised metrics\n",
    "\n",
    "## Regression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "## Classification \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "# Classification Loss Functions (for error)\n",
    "from sklearn.metrics import log_loss, hinge_loss, hamming_loss\n",
    "\n",
    "# We also have our confusion matrix, plots and reports\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Score & Predict on Test Set\n",
    "**When should you bring in the test set?**\n",
    "- Only when you're confident of your model i.e. your scores are consistent\n",
    "- Score your test set, get the predictions, plot residuals or create confusion matrix\n",
    "- **Remember that ANYTHING YOU DO TO YOUR TRAIN DATA, YOU WILL HAVE TO DO TO YOUR TEST DATA!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run your model on your entire dataset\n",
    "\n",
    "Remember to think:\n",
    "\n",
    "**1. Scripts**  \n",
    "**2. Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
